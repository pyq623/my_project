import sys
import json
import json5
import re
import os
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple
import logging
import numpy as np
from datetime import datetime
import pytz
import random
from collections import defaultdict

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).resolve().parent.parent))
from utils import initialize_llm
from configs import get_search_space, get_llm_config, get_tnas_search_space
from models.candidate_models import CandidateModel
from constraints import validate_constraints, ConstraintValidator, MemoryEstimator
from data import get_multitask_dataloaders
from training import MultiTaskTrainer, SingleTaskTrainer
import copy
# åˆå§‹åŒ–é…ç½®
llm_config = get_llm_config()
search_space = get_tnas_search_space()

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('tnas_search.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

dataset_info = \
{
    'har70plus': {
        'name': 'HAR70+',
        'channels': 6,
        'time_steps': 500,
        'num_classes': 7,
        'description': 'Chest (sternum) sensor data with fine-grained activities'
    },
    'motionsense': {
        'name': 'MotionSense',
        'channels': 6,
        'time_steps': 500,
        'num_classes': 6,
        'description': 'Right front pocket sensor data with basic activities'
    },
    'whar': {
        'name': 'w-HAR',
        'channels': 6,
        'time_steps': 2500,
        'num_classes': 7,
        'description': 'Left wrist sensor data with office and daily motions'
    }
}


class DesignPrincipleManager:
    """ç®¡ç†è®¾è®¡åŸåˆ™åŠå…¶å¯¹åº”çš„æœç´¢ç©ºé—´çº¦æŸ"""
    def __init__(self, llm, dataset_name='har70plus'):
        self.llm = llm
        self.principles = []
        self.layer_constraints = {}  # {layer_idx: [allowed_ops]}
        self.performance_history = defaultdict(list)
        self.dataset_insights = None  # ç”¨äºå­˜å‚¨æ•°æ®é›†ç‰¹å®šçš„è§è§£
        self.original_space = search_space  # ç”¨äºå­˜å‚¨åŸå§‹æœç´¢ç©ºé—´
        self.dataset_info = dataset_info  # æ•°æ®é›†ä¿¡æ¯æ˜ å°„
        self.dataset_name = dataset_name  # å½“å‰æ•°æ®é›†åç§°

    def _extract_json_from_response(self, response: str) -> Optional[Dict]:
        """ä»LLMå“åº”ä¸­æå–JSONå†…å®¹"""
        try:
            # å°è¯•åŒ¹é…```json...```æ ¼å¼
            json_match = re.search(r'```json(.*?)```', response, re.DOTALL)
            if json_match:
                json_str = json_match.group(1).strip()
                return json5.loads(json_str)
            
            # å°è¯•åŒ¹é…```...```æ ¼å¼
            json_match = re.search(r'```(.*?)```', response, re.DOTALL)
            if json_match:
                json_str = json_match.group(1).strip()
                return json5.loads(json_str)
            
            # å¦‚æœéƒ½æ²¡æœ‰ä»£ç å—æ ‡è®°ï¼Œå°è¯•ç›´æ¥è§£ææ•´ä¸ªå“åº”
            return json5.loads(response)
        except Exception as e:
            logger.error(f"ä»å“åº”ä¸­æå–JSONå¤±è´¥: {str(e)}")
            logger.debug(f"åŸå§‹å“åº”å†…å®¹: {response}")
            return None
        
    def extract_principles(self, architectures: List[Dict], performances: List[float], sources: List[str]) -> List[str]:
        """
        ä»ä¸€ç»„æ¶æ„ä¸­æå–è®¾è®¡åŸåˆ™
        å‚æ•°:
            architectures: æ¶æ„é…ç½®åˆ—è¡¨
            performances: å¯¹åº”çš„æ€§èƒ½æŒ‡æ ‡åˆ—è¡¨
            sources: æ¶æ„æ¥æºæ•°æ®é›†åˆ—è¡¨
        è¿”å›:
            è®¾è®¡åŸåˆ™åˆ—è¡¨
        """

        # æŒ‰æ•°æ®é›†åˆ†ç»„æ¶æ„å’Œæ€§èƒ½
        dataset_archs = defaultdict(list)
        dataset_perfs = defaultdict(list)

        for arch, perf, source in zip(architectures, performances, sources):
            dataset_archs[source].append(arch)
            dataset_perfs[source].append(perf)

        principles_prompt = """As a neural architecture design expert, analyze the following architectures from different datasets and extract general design principles that can be applied to our target HAR task:
    
        **Current Search Space for HAR:**
        {current_search_space}

        **HAR Task Description:**
        - Input: {har_channels} channels sensor data with {har_timesteps} timesteps
        - Classes: {har_classes}
        - Description: {har_description}
        - Constraints: {har_constraints}

        **Architectures Grouped by Dataset:**
        {datasets_info}
        
        **Steps:**
        1. Analyze the high-performing architectures from source datasets
        2. Identify general design patterns that could apply to HAR
        3. Consider the differences between source tasks and HAR
        4. Propose refined search space constraints that:
            - Are compatible with our current search space
            - Only narrow down (never expand) the options
            - Respect HAR task constraints
        5. For each layer type, suggest allowed operation types from our current options

        **Available Operations in Current Search Space:**
        {available_ops}
        
        **Requirement:**
        1. Strictly follow the given search space and avaliable operations.
        2. Return the schema configuration in JSON format

        **Output Format:**
        {{
            "principles": ["Principle 1", "Principle 2", ...],
            "layer_constraints": {{
                "0": ["DWSepConv", "MBConv"],  # Allowed ops for layer 0
                "1": ["DpConv", "SeSepConv"],  # Allowed ops for layer 1
                ...
            }}
        }}
        """.format(
        current_search_space=json.dumps(self.original_space["search_space"], indent=2),
        har_channels=self.dataset_info[self.dataset_name]["channels"],
        har_timesteps=self.dataset_info[self.dataset_name]["time_steps"],
        har_classes=self.dataset_info[self.dataset_name]["num_classes"],
        har_description=self.dataset_info[self.dataset_name]["description"],
        har_constraints=json.dumps({
            'max_macs': self.original_space["constraints"]["max_macs"],
            'max_params': self.original_space["constraints"]["max_params"],
            'max_sram': self.original_space["constraints"]["max_sram"]
        }, indent=2),
        datasets_info="\n\n".join(
            f"Dataset: {source}\n" + "\n".join(
                f"  Architecture {i+1} (Score: {perfs[i]:.2f}): {json.dumps(arch, indent=4)}"
                for i, arch in enumerate(archs)
            )
            for source, (archs, perfs) in {
                source: (dataset_archs[source], dataset_perfs[source])
                for source in dataset_archs
            }.items()
        ),
        available_ops=", ".join(self.original_space["search_space"]["conv_types"])
    )
        
        try:
            response = self.llm.invoke(principles_prompt).content
            # logger.info(f"LLMåŸå§‹å“åº”: {response}")
            result = self._extract_json_from_response(response)
            print(f"\næå–è®¾è®¡åŸåˆ™å“åº”: {result}\n")
            # éªŒè¯çº¦æŸæ˜¯å¦æœ‰æ•ˆ
            validated_constraints = {}
            for layer, ops in result.get("layer_constraints", {}).items():
                valid_ops = [op for op in ops if op in self.original_space["search_space"]["conv_types"]]
                if valid_ops:
                    validated_constraints[int(layer)] = valid_ops

            self.principles = result.get("principles", [])
            self.layer_constraints = validated_constraints
            # self.layer_constraints = {
            #     int(k): v for k, v in result.get("layer_constraints", {}).items()
            # }

            # å­˜å‚¨æ•°æ®é›†ç‰¹å®šè§è§£
            # self.dataset_insights = result.get("dataset_specific_insights", {})

            return self.principles
        except Exception as e:
            logger.error(f"æå–è®¾è®¡åŸåˆ™å¤±è´¥: {str(e)}")
            return []
    
    def adapt_principles(self, new_architectures: List[Dict], new_performances: List[float], new_sources: List[str]) -> List[str]:
        """
        æ ¹æ®æ–°å‘ç°çš„æ¶æ„è°ƒæ•´è®¾è®¡åŸåˆ™
        å‚æ•°:
            new_architectures: æ–°å‘ç°çš„æ¶æ„åˆ—è¡¨
            new_performances: å¯¹åº”çš„æ€§èƒ½æŒ‡æ ‡åˆ—è¡¨
            new_sources: æ¶æ„æ¥æºæ•°æ®é›†åˆ—è¡¨
        """
        adaptation_prompt = """We have new architectures with their performance from different datasets. Please update the design principles:
    
        **Current Principles:**
        {current_principles}
        
        **New Architectures Grouped by Dataset:**
        {new_architectures}
        
        **Task:**
        1. Analyze why these new architectures perform well/poorly
        2. Compare with previous dataset-specific insights
        3. Update design principles accordingly
        4. Adjust allowed operations for each layer
        5. Keep no more than 3 operations per layer

        **Requirement:**
        1. Strictly follow the given search space and avaliable operations.
        2. Return the schema configuration in JSON format
        
        **Output Format:** 
        {{
            "principles": ["Principle 1", "Principle 2", ...],
            "layer_constraints": {{
                "0": ["DWSepConv", "MBConv"],  # Allowed ops for layer 0
                "1": ["DpConv", "SeSepConv"],  # Allowed ops for layer 1
                ...
            }},
            "dataset_specific_insights": {{
                "har70plus": ["insight 1", ...],
                "motionsense": ["insight 1", ...],
                "whar": ["insight 1", ...]
            }}
        }}
        """.format(
            current_principles="\n".join(f"- {p}" for p in self.principles),
            new_architectures="\n\n".join(
                f"Dataset: {source}\n" + "\n".join(
                    f"  Architecture {i+1} (Accuracy: {new_performances[i]:.2f}): {json.dumps(arch, indent=4)}"
                    for i, arch in enumerate(new_architectures)
                )
                for source in set(new_sources)
            )
        )
        
        try:
            response = self.llm.invoke(adaptation_prompt).content
            # logger.info(f"åŸåˆ™é€‚åº” LLMåŸå§‹å“åº”: {response}")
            result = self._extract_json_from_response(response)
            self.principles = result.get("principles", self.principles)
            self.layer_constraints = {
                int(k): v for k, v in result.get("layer_constraints", self.layer_constraints).items()
            }

            # æ›´æ–°æ•°æ®é›†ç‰¹å®šè§è§£
            self.dataset_insights.update(result.get("dataset_specific_insights", {}))

            return self.principles
        except Exception as e:
            logger.error(f"è°ƒæ•´è®¾è®¡åŸåˆ™å¤±è´¥: {str(e)}")
            return self.principles
    
    def get_refined_search_space(self, original_space: Dict) -> Dict:
        """æ ¹æ®å½“å‰è®¾è®¡åŸåˆ™ç”Ÿæˆç²¾ç‚¼åçš„æœç´¢ç©ºé—´"""
        refined_space = copy.deepcopy(original_space)
        
        # å¦‚æœæ²¡æœ‰ä»»ä½•å±‚çº¦æŸï¼Œç›´æ¥è¿”å›åŸå§‹ç©ºé—´
        if not self.layer_constraints:
            return refined_space

        # è·å–æ‰€æœ‰å¯ç”¨çš„å·ç§¯ç±»å‹
        available_ops = refined_space["search_space"]["conv_types"]

        # é‡‡ç”¨åˆ†å±‚çº¦æŸçš„æ–¹å¼
        # æ·»åŠ åˆ†å±‚çº¦æŸä¿¡æ¯åˆ°æœç´¢ç©ºé—´
        if "layer_constraints" not in refined_space["search_space"]:
            refined_space["search_space"]["layer_constraints"] = {}
        
        for layer_idx, ops in self.layer_constraints.items():
            # ç¡®ä¿æ“ä½œåœ¨åŸå§‹æœç´¢ç©ºé—´å†…
            valid_ops = [op for op in ops if op in available_ops]
            if valid_ops:
                refined_space["search_space"]["layer_constraints"][layer_idx] = valid_ops
        # # åˆ›å»ºä¸€ä¸ªæ–°çš„conv_typesåˆ—è¡¨ï¼ŒåŒ…å«æ‰€æœ‰å±‚å…è®¸çš„æ“ä½œï¼ˆå»é‡ï¼‰
        # all_allowed_ops = set()
        
        # # æ–¹å¼1: é™åˆ¶æ‰€æœ‰å±‚çš„å·ç§¯ç±»å‹é€‰æ‹©
        # for ops in self.layer_constraints.values():
        #     # ç¡®ä¿æ“ä½œåœ¨åŸå§‹æœç´¢ç©ºé—´å†…
        #     valid_ops = [op for op in ops if op in available_ops]
        #     all_allowed_ops.update(valid_ops)
        
        # if all_allowed_ops:
        #     refined_space["search_space"]["conv_types"] = list(all_allowed_ops)
                
        return refined_space

class TNASTransferSearcher:
    """
    æ”¹è¿›åçš„TNASæœç´¢å™¨ï¼ŒåŸºäºè®¾è®¡åŸåˆ™è¿ç§»
    
    å‚æ•°:
        llm_config: LLMé…ç½®å­—å…¸
        search_space: æœç´¢ç©ºé—´å®šä¹‰
        dataset_name: æ•°æ®é›†åç§°
    """
    def __init__(self, llm_config, search_space, dataset_name=['har70plus', 'motionsense', 'whar'], retries=3):
        self.llm = initialize_llm(llm_config)
        self.original_space = search_space
        self.current_space = copy.deepcopy(search_space)
        self.design_manager = DesignPrincipleManager(self.llm)
        self.validator = ConstraintValidator(search_space['constraints'])
        self.dataset_name = dataset_name
        self.performance_history = []
        self.top_archives = []  # ä¿å­˜æ€§èƒ½æœ€å¥½çš„æ¶æ„
        
        # æ•°æ®é›†ä¿¡æ¯æ˜ å°„
        self.dataset_info = dataset_info

        self.retries = retries
        self._failures = []  # åˆå§‹åŒ–å¤±è´¥æ¡ˆä¾‹åˆ—è¡¨

    # def _get_dataset_info(self) -> Dict[str, Any]:
    #     """è·å–å½“å‰æ•°æ®é›†çš„ä¿¡æ¯"""
    #     return self.dataset_info.get(self.dataset_name, self.dataset_info['har70plus'])
    def _get_dataset_info(self) -> Dict[str, Any]:
        """è·å–æ‰€æœ‰æ•°æ®é›†çš„ä¿¡æ¯"""
        return {name: self.dataset_info[name] for name in self.dataset_name}
    
    def _load_initial_architectures(self) -> Tuple[List[Dict], List[float]]:
        """åŠ è½½åˆå§‹æ¶æ„åŠå…¶æ€§èƒ½(ä»ä¸‰ä¸ªNASBench201æ•°æ®é›†)"""
        dataset_files = [
            '/root/tinyml/tnas_background/nasbench201_cifar10.json',
            '/root/tinyml/tnas_background/nasbench201_cifar100.json',
            '/root/tinyml/tnas_background/nasbench201_imagenet.json'
        ]
        
        all_archs = []
        all_perfs = []
        all_sources = []  # è®°å½•æ¯ä¸ªæ¶æ„æ¥è‡ªå“ªä¸ªæ•°æ®é›†
        
        for file_path in dataset_files:
            try:
                # ä»æ–‡ä»¶è·¯å¾„æå–æ•°æ®é›†åç§°
                dataset_name = file_path.split('/')[-1].replace('nasbench201_', '').replace('.json', '')
                with open(file_path, 'r') as f:
                    arch_data = json.load(f)
                
                # æå–å‰25ä¸ªæœ€ä½³æ¶æ„åŠå…¶éªŒè¯å‡†ç¡®ç‡(val_acc_200)
                top_archs = sorted(arch_data.items(), 
                                key=lambda x: x[1]['val_acc_200'], 
                                reverse=True)[:25]
                
                # å°†æ¶æ„å­—ç¬¦ä¸²è½¬æ¢ä¸ºé…ç½®å­—å…¸
                for arch_str, arch_info in top_archs:
                    try:
                        # è§£æNASBench201çš„æ¶æ„å­—ç¬¦ä¸²
                        config = self._parse_nasbench201_arch(arch_str)
                        if config:
                            all_archs.append(config)
                            all_perfs.append(arch_info['val_acc_200'])
                            all_sources.append(dataset_name)
                    except Exception as e:
                        logger.warning(f"è§£ææ¶æ„ {arch_str} å¤±è´¥: {str(e)}")
                        continue
                        
                logger.info(f"ä» {file_path} åŠ è½½äº† {len(top_archs)} ä¸ªæ¶æ„")
            except Exception as e:
                logger.error(f"åŠ è½½ {file_path} å¤±è´¥: {str(e)}")
                continue
        
        # å¦‚æœä¸‰ä¸ªæ–‡ä»¶éƒ½åŠ è½½å¤±è´¥ï¼Œè¿”å›ç©ºåˆ—è¡¨
        if not all_archs:
            return [], []
        
        # ç»Ÿè®¡å„æ•°æ®é›†çš„æ¶æ„æ•°é‡
        source_stats = {name: all_sources.count(name) for name in ['cifar10', 'cifar100', 'imagenet']}
        logger.info(f"åŠ è½½çš„æ¶æ„æ¥æºç»Ÿè®¡: {source_stats}")
        
        # è¿”å›å»é‡åçš„æ¶æ„(ä¿ç•™75ä¸ªï¼Œæ¯ä¸ªæ•°æ®é›†25ä¸ª)
        unique_archs = []
        unique_perfs = []
        unique_sources = []
        seen_archs = set()
        
        for arch, perf, source in zip(all_archs, all_perfs, all_sources):
            arch_str = json.dumps(arch, sort_keys=True)
            if arch_str not in seen_archs:
                seen_archs.add(arch_str)
                unique_archs.append(arch)
                unique_perfs.append(perf)
                unique_sources.append(source)
        
        return unique_archs, unique_perfs, unique_sources

    def _parse_nasbench201_arch(self, arch_str: str) -> Dict:
        """è§£æNASBench201çš„æ¶æ„å­—ç¬¦ä¸²ä¸ºé…ç½®å­—å…¸"""
        # NASBench201æ¶æ„å­—ç¬¦ä¸²ç¤ºä¾‹:
        # |nor_conv_3x3~0|+|nor_conv_1x1~0|nor_conv_1x1~1|+|skip_connect~0|nor_conv_3x3~1|nor_conv_3x3~2|
        
        # åˆå§‹åŒ–é…ç½®å­—å…¸
        config = {
            'stages': {
                '0': {'blocks': []},
                '1': {'blocks': []},
                '2': {'blocks': []}
            }
        }
        
        try:
            # åˆ†å‰²å­—ç¬¦ä¸²è·å–å„é˜¶æ®µä¿¡æ¯
            parts = [p.strip() for p in arch_str.split('+')]
            
            # å¤„ç†æ¯ä¸ªé˜¶æ®µ
            for stage_idx, part in enumerate(parts):
                # ç§»é™¤ç®¡é“ç¬¦å’Œç©ºç™½å­—ç¬¦
                part = part.replace('|', '').strip()
                if not part:
                    continue
                    
                # åˆ†å‰²èŠ‚ç‚¹ - ç°åœ¨æ­£ç¡®å¤„ç†å½¢å¦‚ "nor_conv_3x3~0" çš„èŠ‚ç‚¹
                nodes = [n.strip() for n in part.split() if n.strip()]
                for node in nodes:
                    if not node or '~' not in node:
                        continue
                        
                    # è§£ææ“ä½œå’ŒèŠ‚ç‚¹ç´¢å¼•
                    op, node_idx = node.rsplit('~', 1)
                    config['stages'][str(stage_idx)]['blocks'].append({
                        'type': op,
                        'node_idx': int(node_idx)
                    })
                    
            return config
        except Exception as e:
            logger.error(f"è§£æNASBench201æ¶æ„ {arch_str} å¤±è´¥: {str(e)}")
            return None


    def _initialize_search_space(self):
        """ä½¿ç”¨åˆå§‹æ¶æ„åˆå§‹åŒ–æœç´¢ç©ºé—´"""
        initial_archs, initial_perfs, initial_sources = self._load_initial_architectures()
        if initial_archs:
            self.design_manager.extract_principles(initial_archs, initial_perfs, initial_sources)
            self.current_space = self.design_manager.get_refined_search_space(self.original_space)
            logger.info("åŸºäºåˆå§‹æ¶æ„ç²¾ç‚¼æœç´¢ç©ºé—´å®Œæˆ")

    def _parse_response(self, response: str) -> Optional[CandidateModel]:
        """è§£æLLMå“åº”ä¸ºå€™é€‰æ¨¡å‹"""
        try:
            # å°è¯•è§£æJSONå“åº”
            json_match = re.search(r'```json(.*?)```', response, re.DOTALL)
            if json_match:
                json_str = json_match.group(1).strip()
                config = json5.loads(json_str)
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°jsonä»£ç å—ï¼Œå°è¯•ç›´æ¥è§£ææ•´ä¸ªå“åº”
                json_match = re.search(r'```(.*?)```', response, re.DOTALL)
                if json_match:
                    json_str = json_match.group(1).strip()
                    config = json5.loads(json_str)
                else:
                    # å¦‚æœéƒ½æ²¡æœ‰ä»£ç å—æ ‡è®°ï¼Œå°è¯•ç›´æ¥è§£ææ•´ä¸ªå“åº”
                    config = json5.loads(response.strip())
            
            # logger.info(f"è§£æå‡ºçš„é…ç½®:\n{json.dumps(config, indent=2)}")

            # åŸºæœ¬é…ç½®éªŒè¯
            if not all(k in config for k in ['stages', 'constraints']):
                raise ValueError("é…ç½®ç¼ºå°‘å¿…è¦å­—æ®µ(stages æˆ– constraints)")

            # éå†æ¯ä¸ª stage å’Œ blockï¼Œç¡®ä¿å­—æ®µå®Œæ•´æ€§
            for stage in config.get('stages', []):
                for block in stage.get('blocks', []):
                    if 'type' not in block:
                        raise ValueError(f"Block ç¼ºå°‘ 'type' å­—æ®µ: {block}")
            logger.info("é…ç½®éªŒè¯é€šè¿‡")
            # ç¡®ä¿æ‰€æœ‰æ•°å€¼å­—æ®µéƒ½æ˜¯ æ•°å­—ç±»å‹
            def convert_numbers(obj):
                if isinstance(obj, dict):
                    return {k: convert_numbers(v) for k, v in obj.items()}
                elif isinstance(obj, list):
                    return [convert_numbers(v) for v in obj]
                elif isinstance(obj, str):
                    try:
                        return float(obj) if '.' in obj else int(obj)
                    except ValueError:
                        return obj
                return obj
            logger.debug(f"æ•°å€¼è½¬æ¢æˆåŠŸ")
            config = convert_numbers(config)
            
            # åˆ›å»ºå€™é€‰æ¨¡å‹å®ä¾‹
            return CandidateModel(config=config)
                
        except json.JSONDecodeError as e:
            logger.error(f"æ— æ³•è§£æLLMå“åº”ä¸ºJSON: {str(e)}")
            logger.debug(f"åŸå§‹å“åº”å†…å®¹: {response}")
            return None
        except Exception as e:
            logger.error(f"é…ç½®è§£æå¤±è´¥: {str(e)}")
            return None

    def _record_failure(self, config: Dict, reason: str):
        """è®°å½•å¤±è´¥çš„å€™é€‰æ¶æ„åŠå…¶å¤±è´¥åŸå› """
        if not hasattr(self, "_failures"):
            self._failures = []  # åˆå§‹åŒ–å¤±è´¥æ¡ˆä¾‹åˆ—è¡¨
        self._failures.append({"config": config, "reason": reason})
        logger.info(f"è®°å½•å¤±è´¥æ¡ˆä¾‹: {reason}")

    def generate_candidate(self, feedback: Optional[str] = None) -> Optional[CandidateModel]:
        """ä½¿ç”¨LLMç”Ÿæˆå€™é€‰æ¶æ„ï¼Œæœ‰ self.retries æ¬¡é‡è¯•æœºä¼š"""
        for attempt in range(self.retries):
            include_failures = attempt > 0  # åªåœ¨é‡è¯•æ—¶åŒ…å«å¤±è´¥æ¡ˆä¾‹
            print(f"include_failures: {include_failures}, attempt: {attempt + 1}")
            # æ„å»ºæç¤ºè¯ï¼Œä¼ å…¥åé¦ˆå’Œæ˜¯å¦åŒ…å«å¤±è´¥æ¡ˆä¾‹çš„ä¿¡æ¯
            prompt = self._build_prompt(feedback, include_failures)
            
            try:
                response = self.llm.invoke(prompt).content
                logger.info(f"ç”Ÿæˆæ¶æ„ LLMåŸå§‹å“åº”: {response}")
                candidate = self._parse_response(response)
                if candidate is None:
                    print("âš ï¸ ç”Ÿæˆçš„å€™é€‰æ¶æ„è§£æå¤±è´¥")
                    continue
                # éªŒè¯çº¦æŸ
                is_valid, failure_reason = self._validate_candidate(candidate)
                if is_valid:
                    return candidate

                # å¦‚æœéªŒè¯å¤±è´¥ï¼Œè®°å½•å¤±è´¥æ¡ˆä¾‹å¹¶æä¾›åé¦ˆ
                self._record_failure(candidate.config, failure_reason)
                feedback = f"Failed due to: {failure_reason}"  # æ›´æ–°åé¦ˆä¿¡æ¯
                print("\n----------------------------------------\n")
                print(f"âš ï¸ å°è¯• {attempt + 1} / {self.retries}: ç”Ÿæˆçš„å€™é€‰æ¶æ„ä¸ç¬¦åˆçº¦æŸæ¡ä»¶: {failure_reason}")

            except Exception as e:
                print(f"LLMè°ƒç”¨å¤±è´¥: {str(e)}")
        print(f"âŒ ç»è¿‡ {self.retries} æ¬¡å°è¯•ä»æœªèƒ½ç”Ÿæˆæœ‰æ•ˆæ¶æ„")
        return None

    def _build_prompt(self, feedback: Optional[str] = None, include_failures: bool = False) -> str:
        """æ„å»ºLLMæç¤ºï¼Œæ”¯æŒå¤šä¸ªæ•°æ®é›†"""
        dataset = self._get_dataset_info()

        # è·å–æ‰€æœ‰æ•°æ®é›†çš„æœ€å¤§æ—¶é—´æ­¥é•¿å’Œé€šé“æ•°
        max_time_steps = max(info['time_steps'] for info in dataset_info.values())
        channels = max(info['channels'] for info in dataset_info.values())
        num_classes = max(info['num_classes'] for info in dataset_info.values())

        constraints = {
            'max_sram': float(self.current_space['constraints']['max_sram']) / 1024,  # KB
            'min_macs': float(self.current_space['constraints']['min_macs']) / 1e6,   # M
            'max_macs': float(self.current_space['constraints']['max_macs']) / 1e6,   # M
            'max_params': float(self.current_space['constraints']['max_params']) / 1e6  # M
        }

        # JSONç¤ºä¾‹éƒ¨åˆ†
        json_example = """
        {{
            "input_channels": {input_channels},  
            "num_classes": {num_classes},
            "stages": [
                {{
                    "blocks": [
                        {{
                            "type": "DWSepConv",
                            "kernel_size": 3,
                            "expansion": 3,
                            "has_se": false,
                            "se_ratios": 0,
                            "skip_connection": false,
                            "stride": 1,
                            "activation": "ReLU6"
                        }}
                    ],
                    "channels": 8
                }},
                {{
                    "blocks": [
                        {{
                            "type": "MBConv",
                            "kernel_size": 3,
                            "expansion": 4,
                            "has_se": true,
                            "se_ratios": 0.25,
                            "skip_connection": true,
                            "stride": 2,
                            "activation": "Swish"
                        }}
                    ],
                    "channels": 16
                }},
                {{
                    "blocks": [
                        {{
                            "type": "MBConv",
                            "kernel_size": 5,
                            "expansion": 6,
                            "has_se": true,
                            "se_ratios": 0.25,
                            "skip_connection": true,
                            "stride": 1,
                            "activation": "LeakyReLU"
                        }}
                    ],
                    "channels": 24
                }},
                {{
                    "blocks": [
                        {{
                            "type": "DWSepConv",
                            "kernel_size": 5,
                            "expansion": 3,
                            "has_se": false,
                            "se_ratios": 0,
                            "skip_connection": false,
                            "stride": 2,
                            "activation": "ReLU6"
                        }}
                    ],
                    "channels": 32
                }},
                {{
                    "blocks": [
                        {{
                            "type": "MBConv",
                            "kernel_size": 7,
                            "expansion": 4,
                            "has_se": true,
                            "se_ratios": 0.25,
                            "skip_connection": true,
                            "stride": 1,
                            "activation": "Swish"
                        }}
                    ],
                    "channels": 32
                }}
            ],
            "constraints": {constraints}
        }}
        """.format(
            input_channels=channels,
            num_classes=num_classes,
            constraints=json.dumps(constraints, indent=4)
        )
        logger.info(f"search_space: {self.current_space['search_space']}")

        # æ„å»ºæ•°æ®é›†æè¿°ä¿¡æ¯
        dataset_descriptions = "\n".join(
            f"- {name}: {info['description']} ({info['channels']} channels x {info['time_steps']} steps)"
            for name, info in dataset_info.items()
        )

        # æ„å»ºè®¾è®¡åŸåˆ™éƒ¨åˆ†
        principles_section = "\n**Design Principles:**\n" + "\n".join(f"- {p}" for p in self.design_manager.principles)

        # æ·»åŠ åé¦ˆä¿¡æ¯
        feedback_section = f"\n**Feedback from previous attempts:**\n{feedback}" if feedback else ""

        # æ·»åŠ å¤±è´¥æ¡ˆä¾‹ï¼ˆä»…åœ¨é‡è¯•æ—¶ï¼‰
        failure_section = ""
        if include_failures and hasattr(self, "_failures"):
            failure_section = "\n**Previous Failures:**\n" + "\n".join(
                f"- Reason: {failure['reason']}, Config: {json.dumps(failure['config'], indent=2)}"
                for failure in self._failures
            )

        # **Dataset Info:**
        # - Name: {dataset_name}
        # - Input Shape: (batch_size, {channels}, {time_steps})
        # - Classes: {num_classes}
        # - Description: {description}
            #         dataset_name=dataset['name'],
            # channels=dataset['channels'],
            # time_steps=dataset['time_steps'],
            # num_classes=dataset['num_classes'],
            # description=dataset['description'],

        prompt = """As a neural architecture design expert, generate a new model architecture with these constraints:

        **Current Refined Search Space:**
        {current_search_space}

        **Design Principles:**
        {principles_section}

        **Feedback:**
        {feedback_section}
        {failure_section}

        **Constraints:**
        {constraints}

        **Dataset Information:**
        {dataset_descriptions}

        **Requirements:**
        1. Strictly follow the refined search space and constraints
        2. For each layer, only use allowed operations
        3. Return JSON configuration with complete stages/blocks
        4. For HAR time-series data, use only Conv1D operations
        5. For DWSepConv, DpConv, and SeDpConv convolutions, their out channel should be able to be divided by the in channel (Because they have a 'group' parameter.).
        6. For DWSepConv, DpConv, and SeDpConv convolutions, their out channel should be able to be divided by the in channel!!! (Repeated for emphasis)

        **Notion**
        1. Each block must contain ONLY ONE convolution operation
        2. Multiple operations should be distributed across different blocks
        3. Follow the exact JSON format shown below

        **Output Format:** JSON configuration only.
        {json_example}
        """.format(
            current_search_space=json.dumps(self.current_space['search_space'], indent=2),
            principles_section=principles_section,
            feedback_section=feedback_section,
            failure_section=failure_section,
            constraints=json.dumps(constraints, indent=2),
            dataset_descriptions=dataset_descriptions,
            json_example=json_example
        )

        return prompt

    def _validate_candidate(self, candidate: CandidateModel) -> bool:
        """éªŒè¯å€™é€‰æ¨¡å‹å¹¶è¿”å›éªŒè¯ç»“æœ"""
        print(f"å¼€å§‹æ¶æ„æ£€éªŒ")
        violations = []
        
        # Check MACs constraint
        # macs = float(candidate.estimate_macs())
        # min_macs = float(self.original_space['constraints']['min_macs'])/1e6
        # max_macs = float(self.original_space['constraints']['max_macs'])/1e6
        # if macs < min_macs:
        #     violations.append(f"MACs {macs:.2f}M < min {min_macs:.2f}M")
        # elif macs > max_macs:
        #     violations.append(f"MACs {macs:.2f}M > max {max_macs:.2f}M")
        
        # Check SRAM constraint
        sram = MemoryEstimator.calc_model_sram(candidate)
        max_sram = float(self.original_space['constraints']['max_sram'])
        if sram > max_sram:
            violations.append(f"SRAM {sram/1e3:.1f}KB > max {max_sram/1e3:.1f}KB")
        logger.info(f"SRAMä¼°è®¡: {sram/1e3:.1f}KB, æœ€å¤§é™åˆ¶: {max_sram/1e3:.1f}KB")
        # Check Params constraint
        params = float(candidate.estimate_params())
        max_params = float(self.original_space['constraints']['max_params']) / 1e6
        if params > max_params:
            violations.append(f"Params {params:.2f}M > max {max_params:.2f}M")
        logger.info(f"å‚æ•°ä¼°è®¡: {params:.2f}M, æœ€å¤§é™åˆ¶: {max_params:.2f}M")

        # è·å–å…¨å±€è¾“å…¥é€šé“æ•°
        input_channels = candidate.config.get("input_channels")
        if input_channels is None:
            violations.append("Missing 'input_channels' in candidate configuration")
            return False, "; ".join(violations)

        # éå†æ¯ä¸ª stage å’Œ blockï¼Œæ¨å¯¼ in_channels å’Œ out_channels
        current_in_channels = input_channels  # åˆå§‹è¾“å…¥é€šé“æ•°
        for stage_idx, stage in enumerate(candidate.config.get("stages", [])):
            stage_channels = stage.get("channels")
            if stage_channels is None:
                violations.append(f"Stage {stage_idx}: Missing 'channels'")
                continue

            for block_idx, block in enumerate(stage.get("blocks", [])):
                block_type = block.get("type", "")
                if block_type in ["DWSepConv", "DpConv", "SeDpConv"]:
                    # æ£€æŸ¥ block çš„è¾“å…¥å’Œè¾“å‡ºé€šé“æ˜¯å¦æ»¡è¶³çº¦æŸ
                    if current_in_channels is None or stage_channels is None:
                        violations.append(
                            f"Stage {stage_idx}, Block {block_idx}: Missing 'in_channels' or 'out_channels' "
                            f"for {block_type} (derived from stage)"
                        )
                        continue

                    # æ£€æŸ¥ out_channels æ˜¯å¦èƒ½è¢« in_channels æ•´é™¤
                    if stage_channels % current_in_channels != 0:
                        violations.append(
                            f"Stage {stage_idx}, Block {block_idx}: 'out_channels' ({stage_channels}) "
                            f"is not divisible by 'in_channels' ({current_in_channels}) for {block_type}"
                        )

                # æ›´æ–°å½“å‰è¾“å…¥é€šé“æ•°ä¸ºä¸‹ä¸€ block çš„è¾“å…¥
                current_in_channels = stage_channels


        print(f"æ¶æ„æ£€éªŒå®Œæˆ, çº¦æŸæ£€æŸ¥ç»“æœ: {violations}")
        if violations:
            failure_reason = "; ".join(violations)
            logger.warning(f"å€™é€‰æ¶æ„è¿åçº¦æŸ: {'; '.join(violations)}")
            return False, failure_reason
        
        return True, None

    def run_evolution_search(self, iterations: int = 100) -> Dict:
        """
        è¿è¡Œæ”¹è¿›åçš„TNASæœç´¢æµç¨‹
        å‚æ•°:
            iterations: æœç´¢è¿­ä»£æ¬¡æ•°
        è¿”å›:
            åŒ…å«æœ€ä½³æ¨¡å‹å’Œè®¾è®¡åŸåˆ™çš„å­—å…¸
        """
        # 1. åˆå§‹åŒ–æœç´¢ç©ºé—´
        self._initialize_search_space()
        # print(f"åˆå§‹æœç´¢ç©ºé—´: {json.dumps(self.current_space['search_space'], indent=2)}")
        # 2. å‡†å¤‡æ•°æ®é›†
        dataset_info = self._get_dataset_info()

        # è·å–æ‰€æœ‰æ•°æ®é›†çš„æœ€å¤§æ—¶é—´æ­¥é•¿å’Œé€šé“æ•°
        max_time_steps = max(info['time_steps'] for info in dataset_info.values())
        channels = max(info['channels'] for info in dataset_info.values())
        input_shape = (1, channels, max_time_steps)


        dataloaders = get_multitask_dataloaders('/root/tinyml/data')

        # 3. åˆ›å»ºä¿å­˜ç›®å½•
        china_tz = pytz.timezone("Asia/Shanghai")
        base_dir = "/root/tinyml/weights/tnas"
        os.makedirs(base_dir, exist_ok=True)
        run_dir = os.path.join(base_dir, datetime.now(china_tz).strftime("%m-%d-%H-%M"))
        os.makedirs(run_dir, exist_ok=True)
        logger.info(f"ä¿å­˜ç›®å½•: {run_dir}")

        best_accuracy = 0.0
        best_model = None
        best_val_accuracy = None
        all_candidates = []  # åˆå§‹åŒ–ç”¨äºå­˜å‚¨æ‰€æœ‰å€™é€‰æ¶æ„çš„ä¿¡æ¯
        
        for i in range(iterations):
            logger.info(f"\n{'='*30} Iteration {i+1}/{iterations} {'='*30}")
            
            # ç”Ÿæˆå€™é€‰æ¶æ„
            candidate = self.generate_candidate()
            if not candidate:
                continue

            try:
                # è®­ç»ƒå’Œè¯„ä¼°
                model = candidate.build_model()
                print(f"æ¨¡å‹æ„å»ºæˆåŠŸ")

                try:
                    from torchinfo import summary
                    summary(model, input_size=input_shape)
                except ImportError:
                    print("âš ï¸ æœªå®‰è£…torchinfoï¼Œ æ— æ³•æ‰“å°æ¨¡å‹ç»“æ„")
                    print("æ¨¡å‹ç»“æ„:", model)

                trainer = MultiTaskTrainer(model, dataloaders)
                save_path = os.path.join(run_dir, f"model_iter_{i+1}.pth")
                # è¿™é‡Œçš„bestæ˜¯æŒ‡å¤šæ¬¡epochsè®­ç»ƒä¸­ç›¸åŒæ¶æ„å¤šä¸ªæƒé‡ä¸­å¯¹åº”çš„æœ€ä½³éªŒè¯å‡†ç¡®ç‡
                accuracy, val_metrics, history, best_state = trainer.train(epochs=10, save_path=save_path)
                
                # è®¡ç®—å€™é€‰æ¶æ„çš„å…¶ä»–æŒ‡æ ‡
                sram = MemoryEstimator.calc_model_sram(candidate)  # SRAM
                params = candidate.estimate_params()  # å‚æ•°é‡
                latency = candidate.measure_latency(device='cuda')  # æ¨ç†å»¶è¿Ÿ

                # **æ–°å¢ï¼šè®¡ç®—å³°å€¼å†…å­˜**
                try:
                    peak_memory_mb = candidate.measure_peak_memory(device='cuda')  # é»˜è®¤ä½¿ç”¨ GPU
                except Exception as e:
                    logger.warning(f"æ— æ³•åœ¨ GPU ä¸Šæµ‹é‡å³°å€¼å†…å­˜ï¼Œå°è¯•ä½¿ç”¨ CPU: {str(e)}")
                    peak_memory_mb = candidate.measure_peak_memory(device='cpu')  # å›é€€åˆ° CPU
                logger.info(f"â±ï¸ æ¨ç†å»¶è¿Ÿ: {latency:.2f} ms, å³°å€¼å†…å­˜: {peak_memory_mb:.2f} MB")

                # æ›´æ–°å€™é€‰æ¶æ„çš„æŒ‡æ ‡
                candidate.accuracy = accuracy
                candidate.sram = sram
                candidate.params = params
                candidate.latency = latency
                candidate.val_accuracy = val_metrics
                candidate.peak_memory = peak_memory_mb  # **æ–°å¢ï¼šå³°å€¼å†…å­˜**

                # ä¿å­˜å€™é€‰æ¶æ„çš„å®Œæ•´ä¿¡æ¯
                candidate_info = {
                    "config": candidate.config,
                    "metrics": {
                        "accuracy": accuracy,
                        "sram": sram,
                        "params": params,
                        "latency": latency,
                        "peak_memory": peak_memory_mb,  # **æ–°å¢ï¼šå³°å€¼å†…å­˜**
                        "val_accuracy": val_metrics
                    },
                    "save_path": save_path
                }
                all_candidates.append(candidate_info)  # ä¿å­˜åˆ°å€™é€‰æ¶æ„åˆ—è¡¨

                # è®°å½•æ€§èƒ½
                self.performance_history.append((candidate.config, accuracy, val_metrics))
                self.top_archives.append((candidate, accuracy))
                self.top_archives.sort(key=lambda x: x[1], reverse=True)
                if len(self.top_archives) > 20:
                    self.top_archives = self.top_archives[:20]
                
                # æ›´æ–°æœ€ä½³æ¨¡å‹
                if accuracy > best_accuracy:
                    best_accuracy = accuracy
                    best_model = candidate
                    best_val_accuracy = val_metrics
                    logger.info(f"ğŸ† æ–°æœ€ä½³æ¨¡å‹! å‡†ç¡®ç‡: {accuracy:.2f}% sram: {candidate.sram} latency: {candidate.latency}ms")
                
                # æ¯ 10 æ¬¡è¿­ä»£è°ƒæ•´è®¾è®¡åŸåˆ™
                if i > 0 and i % 3 == 0:
                    self._adapt_search_space()
                    
            except Exception as e:
                logger.error(f"æ¨¡å‹è¯„ä¼°å¤±è´¥: {str(e)}")
                continue

        # æ‰“å°æœ€ä½³æ¶æ„çš„è¯¦ç»†ä¿¡æ¯
        if best_model:
            logger.info("\næœ€ä½³æ¶æ„è¯¦ç»†ä¿¡æ¯:")
            best_config = best_model.config
            sram = MemoryEstimator.calc_model_sram(best_model)
            params = float(best_model.estimate_params())  # è½¬æ¢ä¸ºç™¾ä¸‡å‚æ•°
            logger.info(f"æœ€ä½³æ¶æ„é…ç½®:\n{json.dumps(best_config, indent=2)}")
            logger.info(f"SRAM: {sram / 1e3:.1f} KB")
            logger.info(f"å‚æ•°é‡: {params:.2f} M")
            logger.info(f"å‡†ç¡®ç‡: {best_accuracy:.2f}%")
            logger.info(f"æ¨ç†å»¶è¿Ÿ: {best_model.measure_latency(device='cuda'):.2f} ms")
            logger.info(f"å³°å€¼å†…å­˜: {best_model.peak_memory:.2f} MB")  # **æ–°å¢ï¼šå³°å€¼å†…å­˜**
            # æ ¼å¼åŒ–éªŒè¯å‡†ç¡®ç‡
            if isinstance(best_val_accuracy, dict):
                formatted_val_accuracy = ", ".join(
                    f"{dataset}: {metrics['accuracy']:.2f}%" for dataset, metrics in best_val_accuracy.items()
                )
                logger.info(f"éªŒè¯å‡†ç¡®ç‡: {formatted_val_accuracy}")
            else:
                logger.info(f"éªŒè¯å‡†ç¡®ç‡: {best_val_accuracy:.2f}%")
            
        else:
            logger.info("æœªæ‰¾åˆ°æœ€ä½³æ¶æ„")
            print("æœªæ‰¾åˆ°æœ€ä½³æ¶æ„")
        
        # ä¿å­˜æ‰€æœ‰å€™é€‰æ¶æ„ä¿¡æ¯åˆ°æ–‡ä»¶
        try:
            candidates_save_path = os.path.join(run_dir, "all_candidates.json")
            with open(candidates_save_path, 'w', encoding='utf-8') as f:
                json.dump(all_candidates, f, indent=2, ensure_ascii=False)
            logger.info(f"æ‰€æœ‰å€™é€‰æ¶æ„ä¿¡æ¯å·²ä¿å­˜åˆ°: {candidates_save_path}")
        except Exception as e:
            logger.error(f"ä¿å­˜å€™é€‰æ¶æ„ä¿¡æ¯å¤±è´¥: {str(e)}")

        # ä¿å­˜æœ€ä½³æ¶æ„ä¿¡æ¯åˆ°æ–‡ä»¶
        try:
            best_model_save_path = os.path.join(run_dir, "best_model.json")
            with open(best_model_save_path, 'w', encoding='utf-8') as f:
                json.dump({
                    "config": best_model.config if best_model else None,
                    "metrics": {
                        "accuracy": best_accuracy,
                        "sram": best_model.sram if best_model else None,
                        "params": best_model.params if best_model else None,
                        "latency": best_model.latency if best_model else None,
                        "peak_memory": best_model.peak_memory if best_model else None,
                        "val_accuracy": best_val_accuracy
                    }
                }, f, indent=2, ensure_ascii=False)
            logger.info(f"æœ€ä½³æ¶æ„ä¿¡æ¯å·²ä¿å­˜åˆ°: {best_model_save_path}")
        except Exception as e:
            logger.error(f"ä¿å­˜æœ€ä½³æ¶æ„ä¿¡æ¯å¤±è´¥: {str(e)}")


        return {
            'best_model': best_model,
            'best_accuracy': best_accuracy,
            'design_principles': self.design_manager.principles,
            'top_archives': self.top_archives,
            'best_val_accuracy': best_val_accuracy
        }

    def _adapt_search_space(self):
        """æ ¹æ®æœç´¢å†å²è°ƒæ•´æœç´¢ç©ºé—´"""
        if len(self.performance_history) < 5:
            return
            
        # æå–æœ€è¿‘çš„é«˜æ€§èƒ½æ¶æ„åŠå…¶æ€§èƒ½
        recent_data = sorted(self.performance_history[-20:], key=lambda x: x[1], reverse=True)[:5]
        recent_archs = [x[0] for x in recent_data]
        recent_perfs = [x[1] for x in recent_data]

        # ä½¿ç”¨å½“å‰æ•°æ®é›†åç§°ä½œä¸ºæ¥æº
        recent_sources = [self.dataset_name] * len(recent_archs)
        
        # è°ƒæ•´è®¾è®¡åŸåˆ™
        self.design_manager.adapt_principles(recent_archs, recent_perfs, recent_sources)
        
        # æ›´æ–°æœç´¢ç©ºé—´
        self.current_space = self.design_manager.get_refined_search_space(self.original_space)
        logger.info("æœç´¢ç©ºé—´å·²åŸºäºæ–°å‘ç°æ¶æ„è°ƒæ•´")

if __name__ == "__main__":
    # ç¤ºä¾‹ç”¨æ³•
    searcher = TNASTransferSearcher(llm_config["llm"], search_space)
    
    # è¿è¡Œæœç´¢(ä¼šè‡ªåŠ¨åŠ è½½åˆå§‹æ¶æ„å¹¶æå–è®¾è®¡åŸåˆ™)
    results = searcher.run_evolution_search(iterations=10)
    
    print(f"\næœç´¢å®Œæˆ. æœ€ä½³å‡†ç¡®ç‡: {results['best_accuracy']:.2f}%")
    print(f"è®¾è®¡åŸåˆ™: {results['design_principles']}")