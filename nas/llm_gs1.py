import openai  # æˆ–å…¶ä»– LLM API
import sys
import json5
import json
from pathlib import Path
from typing import Dict, Any, Optional, Tuple, List
import re
sys.path.append(str(Path(__file__).resolve().parent.parent))  # æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
from utils import initialize_llm  # ä¿®æ”¹å¯¼å…¥è·¯å¾„
# ä»configså¯¼å…¥æç¤ºæ¨¡æ¿
from configs import get_search_space, get_llm_config, get_tnas_search_space
# å¯¼å…¥æ¨¡å‹å’Œçº¦æŸéªŒè¯ç›¸å…³æ¨¡å—
from models.candidate_models import CandidateModel
from constraints import validate_constraints, ConstraintValidator, MemoryEstimator
from pareto_optimization import ParetoFront
from data import get_multitask_dataloaders
from training import MultiTaskTrainer, SingleTaskTrainer
import logging
import numpy as np
import os
from datetime import datetime
import pytz

llm_config = get_llm_config()
# search_space = get_search_space()
search_space = get_tnas_search_space()

# åœ¨æ–‡ä»¶å¼€å¤´æ·»åŠ 
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('nas_search.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

class LLMGuidedSearcher:
    """
    LLMå¼•å¯¼çš„ç¥ç»ç½‘ç»œæ¶æ„æœç´¢å™¨
    
    å‚æ•°:
        llm_config: LLMé…ç½®å­—å…¸
        search_space: æœç´¢ç©ºé—´å®šä¹‰
    """
    # , 'MotionSense', 'w-HAR', 'WISDM', 'Harth', 'USCHAD', 'UTD-MHAD', 'DSADS'
    def __init__(self, llm_config, search_space, dataset_names=['har70plus', 'MotionSense']):
        self.llm = initialize_llm(llm_config)
        self.search_space = search_space
        # åˆå§‹åŒ–Paretoå‰æ²¿
        self.pareto_front = ParetoFront(constraints=search_space['constraints'])
        self.retries = 3  # é‡è¯•æ¬¡æ•°
        # å­˜å‚¨æœ€è¿‘å¤±è´¥çš„å€™é€‰æ¶æ„
        self.recent_failures: List[Tuple[Dict, str]] = []
        # åˆå§‹åŒ–çº¦æŸéªŒè¯å™¨
        self.validator = ConstraintValidator(search_space['constraints'])

        self.dataset_names = dataset_names
        self.dataset_info = {
            name: self._load_dataset_info(name) for name in dataset_names
        }

    def _load_dataset_info(self, name):
        info = {
            'har70plus': {
                'channels': 6, 
                'time_steps': 500, 
                'num_classes': 7,
                'description': 'Chest (sternum) sensor data, including fine-grained daily activities such as brushing teeth and chopping vegetables'
            },
            'MotionSense': {
                'channels': 6, 
                'time_steps': 500, 
                'num_classes': 6,
                'description': 'Front right trouser pocket sensor data, including basic activities such as walking, jogging and climbing stairs'
            },
            'w-HAR': {
                'channels': 6, 
                'time_steps': 2500, 
                'num_classes': 7,
                'description': 'Left wrist sensor data, including walking, running, jumping and other office and daily movements'
            },
            'WISDM':{
                'channels': 6,
                'time_steps': 200,
                'num_classes': 18,
                'description': 'A set of data collected based on sensors placed in pants pockets and wrists, including fine-grained actions such as walking, running, going up and down stairs, sitting and standing.'
            },
            'Harth':{
                'channels': 6,
                'time_steps': 500,
                'num_classes': 12,
                'description': 'A set of sensor data based on the right thigh and lower back, including cooking/cleaning, Yoga/weight lifting, walking on the flat/stairs, etc.'
            },
            'USCHAD': {
                'channels': 6,
                'time_steps': 1000,
                'num_classes': 12,
                'description': 'A group of sensing data based on the right front hip, including walking, running, going upstairs, going downstairs, jumping, sitting, standing, sleeping and taking the elevator.'
            },
            'UTD-MHAD': {
                'channels': 6,
                'time_steps': 300,
                'num_classes': 27,
                'description': 'A group of sensing data based on the right wrist or right thigh, including waving, punching, clapping, jumping, push ups and other actions.'
            },
            'DSADS': {
                'channels': 45,
                'time_steps': 125,
                'num_classes': 19,
                'description': 'A group of sensing data based on trunk, right arm, left arm, right leg and left leg, including whole body and local actions such as sitting and relaxing, using computer'
            },
            'DSADS1': {
                'channels': 45,
                'time_steps': 2500,
                'num_classes': 19,
                'description': 'A group of sensing data based on trunk, right arm, left arm, right leg and left leg, including whole body and local actions such as sitting and relaxing, using computer'
            },
            'w-HAR1': {
                'channels': 45, 
                'time_steps': 2500, 
                'num_classes': 7,
                'description': 'Left wrist sensor data, including walking, running, jumping and other office and daily movements'
            }
        }
        return info[name]

        
    def generate_candidate(self, dataset_name: str, feedback: Optional[str] = None) -> Optional[CandidateModel]:
        """
        ä½¿ç”¨LLMç”Ÿæˆå€™é€‰æ¶æ„ï¼ŒåŸºäºç‰¹å®šæ•°æ®é›†çš„ä¿¡æ¯
        å‚æ•°:
            dataset_name: å½“å‰æ•°æ®é›†çš„åç§°
            feedback: ä¸Šä¸€æ¬¡çš„åé¦ˆä¿¡æ¯
        è¿”å›:
            ä¸€ä¸ªå€™é€‰æ¨¡å‹
        """
        for attempt in range(self.retries):
            include_failures = attempt > 0  # åªåœ¨é‡è¯•æ—¶åŒ…å«å¤±è´¥æ¡ˆä¾‹
            # æ„å»ºæç¤ºè¯
            print(f"include_failures: {include_failures}, attempt: {attempt + 1}")

            prompt = self._build_prompt(dataset_name, feedback, include_failures)

            try:
                # è°ƒç”¨ LLM ç”Ÿæˆå“åº”
                response = self.llm.invoke(prompt).content
                print(f"LLMåŸå§‹å“åº”:\n{response[50:]}\n{'-'*50}")
                
                # è§£æå“åº”å¹¶éªŒè¯çº¦æŸ
                candidate = self._parse_response(response)
                if candidate is None:
                    print("âš ï¸ ç”Ÿæˆçš„å€™é€‰æ¶æ„ä¸ç¬¦åˆçº¦æŸæ¡ä»¶")
                    continue
                # éªŒè¯çº¦æŸ
                is_valid, failure_reason, suggestions  = self._validate_candidate(candidate, dataset_name)
                if is_valid:
                    return candidate
                
                # è®°å½•å¤±è´¥æ¡ˆä¾‹
                self._record_failure(candidate.config, failure_reason, suggestions)
                print("\n----------------------------------------\n")
                print(f"âš ï¸ å°è¯• {attempt + 1} / {self.retries}: ç”Ÿæˆçš„å€™é€‰æ¶æ„ä¸ç¬¦åˆçº¦æŸæ¡ä»¶: {failure_reason}")
                print(f"ä¼˜åŒ–å»ºè®®:\n{suggestions}")

            except Exception as e:
                print(f"LLMè°ƒç”¨å¤±è´¥: {str(e)}")

        print(f"âŒ ç»è¿‡ {self.retries} æ¬¡å°è¯•ä»æœªèƒ½ç”Ÿæˆæœ‰æ•ˆæ¶æ„")
        return None

    def _validate_candidate(self, candidate: CandidateModel, dataset_name: str) -> Tuple[bool, str]:
        """éªŒè¯å€™é€‰æ¨¡å‹å¹¶è¿”å›æ‰€æœ‰å¤±è´¥åŸå› """
        violations = []
        suggestions = []
        
        # Check MACs constraint
        macs = float(candidate.estimate_macs())
        min_macs = float(self.search_space['constraints']['min_macs'])/1e6
        max_macs = float(self.search_space['constraints']['max_macs'])/1e6
        macs_status = f"MACs: {macs:.2f}M"
        if macs < min_macs:
            macs_status += f" (Below the minimum value {min_macs:.2f}M)"
            violations.append(macs_status)
            suggestions.append("- Increase the expansion ratio in MBConv\n"
                               "- Add more blocks to increase computation")
        elif macs > max_macs:
            macs_status += f" (Exceeding the maximum value {max_macs:.2f}M)"
            violations.append(macs_status)
            suggestions.append("- Reduce the number of blocks\n"
                               "- Decrease the expansion ratio in MBConv"
                               "- Use more stride=2 downsampling\n"
                               "- Reduce channels in early layers")
        else:
            macs_status += " (Compliant with constraints)"
        
        # Check SRAM constraint
        sram = MemoryEstimator.calc_model_sram(candidate)
        max_sram = float(self.search_space['constraints']['max_sram'])
        sram_status = f"SRAM: {float(sram)/1e3:.1f}KB"
        if sram > max_sram:
            sram_status += f" (Exceeding the maximum value {max_sram/1e3:.1f}KB)"
            violations.append(sram_status)
            suggestions.append("- Reduce model size by removing redundant blocks\n"
                               "- Optimize channel distribution")
        else:
            sram_status += " (Compliant with constraints)"
        
        # Check Params constraint
        params = float(candidate.estimate_params())
        max_params = float(self.search_space['constraints']['max_params']) / 1e6
        params_status = f"Params: {params:.2f}M"
        if params > max_params:
            params_status += f" (Exceeding the maximum value {max_params:.2f}M)"
            violations.append(params_status)
            suggestions.append("- Reduct the number of stages\n"
                               "- Reduce the number of channels or blocks\n"
                               "- Use lightweight operations like depthwise separable convolutions")
        else:
            params_status += " (Compliant with constraints)"
        
        # Check Peak Memory constraint
        peak_memory = candidate.measure_peak_memory(device='cuda', dataset_names=dataset_name)
        max_peak_memory = float(self.search_space['constraints'].get('max_peak_memory', float('inf'))) / 1e6  # é»˜è®¤æ— é™åˆ¶
        peak_memory_status = f"Peak Memory: {peak_memory:.2f}MB"
        if peak_memory > max_peak_memory:
            peak_memory_status += f" (Exceeding the maximum value {max_peak_memory:.2f}MB)"
            violations.append(peak_memory_status)
            suggestions.append("- Reduct the number of stages (if there are 5 stages, you can use less!!!)\n"
                               "- Reduce model size by removing redundant blocks\n"
                               "- Reduce channel distribution in later stages\n"
                               "- Use more efficient pooling layers\n"
                               "- Consider quantization or pruning")
        else:
            peak_memory_status += " (Compliant with constraints)"

        # Check Latency constraint
        latency = candidate.measure_latency(device='cuda', dataset_names=dataset_name)
        max_latency = float(self.search_space['constraints'].get('max_latency', float('inf')))  # é»˜è®¤æ— é™åˆ¶
        latency_status = f"Latency: {latency:.2f}ms"
        if latency > max_latency:
            latency_status += f" (Exceeding the maximum value {max_latency:.2f}ms)"
            violations.append(latency_status)
            suggestions.append("- Optimize convolution operations\n"
                               "- Reduce the number of blocks in each stage\n"
                               "- Use depthwise separable convolutions\n"
                               "- Consider model quantization")
        else:
            latency_status += " (Compliant with constraints)"

        # Print all metrics
        print("\n---- çº¦æŸéªŒè¯ç»“æœ ----")
        print(macs_status)
        print(sram_status)
        print(params_status)
        print(peak_memory_status)
        print(latency_status)
        print("----------------------")
        
        if violations:
            # return False, " | ".join(violations)
            failure_reason = " | ".join(violations)
            optimization_suggestions = "\n".join(suggestions)
            # self._record_failure(candidate.config, failure_reason)
            return False, failure_reason, optimization_suggestions
        return True, ""


    def _record_failure(self, config: Dict, reason: str, suggestions: Optional[str] = None):
        """è®°å½•å¤±è´¥çš„å€™é€‰æ¶æ„"""
        failure_entry = {
            "config": config,
            "reason": reason,
            "suggestions": suggestions or "No specific suggestions"
        }
        self.recent_failures.append(failure_entry)
        # åªä¿ç•™æœ€è¿‘çš„ self.retries ä¸ªå¤±è´¥æ¡ˆä¾‹
        if len(self.recent_failures) > self.retries:
            self.recent_failures.pop(0)
    
    def _build_prompt(self, dataset_name: str, feedback: Optional[str], include_failures: bool) -> str:
        """
        æ„å»ºLLMæç¤ºï¼ŒåŸºäºç‰¹å®šæ•°æ®é›†çš„ä¿¡æ¯
        å‚æ•°:
            dataset_name: å½“å‰æ•°æ®é›†çš„åç§°
            feedback: ä¸Šä¸€æ¬¡çš„åé¦ˆä¿¡æ¯
            include_failures: æ˜¯å¦åŒ…å«å¤±è´¥æ¡ˆä¾‹
        """
        dataset_info = self.dataset_info[dataset_name]
        # ä»Paretoå‰æ²¿è·å–åé¦ˆ(å¦‚æœæœªæä¾›)
        if feedback is None:
            feedback = self.pareto_front.get_feedback()

        # ä»æœç´¢ç©ºé—´è·å–çº¦æŸæ¡ä»¶ï¼Œå¹¶ç¡®ä¿æ•°å€¼æ˜¯ int/float
        constraints = {
            'max_sram': float(self.search_space['constraints']['max_sram']) / 1024,  # è½¬æ¢ä¸ºKB
            'min_macs': float(self.search_space['constraints']['min_macs']) / 1e6,   # è½¬æ¢ä¸ºM
            'max_macs': float(self.search_space['constraints']['max_macs']) / 1e6,   # è½¬æ¢ä¸ºM
            'max_params': float(self.search_space['constraints']['max_params']) / 1e6,  # è½¬æ¢ä¸ºM
            'max_peak_memory': float(self.search_space['constraints']['max_peak_memory']) / 1e6,  # è½¬æ¢ä¸ºMB  é»˜è®¤200MB
            'max_latency': float(self.search_space['constraints']['max_latency']) 
        }

        print(f"\nfeedback: {feedback}\n")

        # æ„å»ºå¤±è´¥æ¡ˆä¾‹åé¦ˆéƒ¨åˆ†
        failure_feedback = ""
        if include_failures and self.recent_failures:
            failure_feedback = "\n**Recent failed architecture cases, reasons and suggestions:**\n"
            for i, failure in enumerate(self.recent_failures, 1):
                failure_feedback += f"{i}. architecture: {json.dumps(failure['config'], indent=2)}\n"
                failure_feedback += f"   reason: {failure['reason']}\n"
                failure_feedback += f"   suggestion: {failure['suggestions']}\n\n"


        search_prompt = """As a neural network architecture design expert, please generate a new tiny model architecture based on the following constraints and search space:

        **Constraints:**
        {constraints}

        **Search Space:**
        {search_space}

        **Feedback:**
        {feedback}

        **Recent failed architecture cases:**
        {failure_feedback}

        **Dataset Information:**
        - Name: {dataset_name}
        - Input Shape: (batch_size, {channels}, {time_steps})
        - Number of Classes: {num_classes}
        - Description: {description}

        **Important Notes:**
        - All convolutional blocks must use 1D operations (Conv1D) for HAR time-series data processing.
        - If has_se is set to False, then se_ratios will be considered as 0, and vice versa. Conversely, if Has_se is set to True, then se_ratios must be greater than 0, and the same holds true in reverse.
        - In the search space, "DWSepConv" and "MBConv" both refer to "DWSepConv1D" and "MBConv1D", but when you generate the configuration, you should only write "DWSepConv" and "MBConv" according to the instructions in the search space.
        - Must support {num_classes} output classes
        - In the format example, I used five blocks, but in fact, it can not be five blocks, it can be any number.

        **Task:**
        You need to design a model architecture capable of processing a diverse range of time series data for human activity recognition (HAR). 
        

        **Requirement:**
        1. Strictly follow the given search space and constraints.
        2. Return the schema configuration in JSON format
        3. Includes complete definitions of stages and blocks.
        4. If there are failure cases and the reason for failure is exceeding limits, then immediately reduce the parameters or reduce the block. Conversely, increase them.

        Here is the format example for the architecture configuration if the input channels is 6 and num_classes is 7. (by the way, the example architecture's peak memory is over 130MB.)
        **Return format example:**
        {{
            "input_channels": 6,  
            "num_classes": 7,
            "stages": [
                {{
                    "blocks": [
                        {{
                            "type": "DWSepConv",
                            "kernel_size": 3,
                            "expansion": 3,
                            "has_se": false,
                            "se_ratios": 0,
                            "skip_connection": false,
                            "stride": 1,
                            "activation": "ReLU6"
                        }}
                    ],
                    "channels": 8
                }},
                {{
                    "blocks": [
                        {{
                            "type": "MBConv",
                            "kernel_size": 3,
                            "expansion": 4,
                            "has_se": true,
                            "se_ratios": 0.25,
                            "skip_connection": true,
                            "stride": 2,
                            "activation": "Swish"
                        }}
                    ],
                    "channels": 16
                }},
                {{
                    "blocks": [
                        {{
                            "type": "MBConv",
                            "kernel_size": 5,
                            "expansion": 6,
                            "has_se": true,
                            "se_ratios": 0.25,
                            "skip_connection": true,
                            "stride": 1,
                            "activation": "LeakyReLU"
                        }}
                    ],
                    "channels": 24
                }},
                {{
                    "blocks": [
                        {{
                            "type": "DWSepConv",
                            "kernel_size": 5,
                            "expansion": 3,
                            "has_se": false,
                            "se_ratios": 0,
                            "skip_connection": false,
                            "stride": 2,
                            "activation": "ReLU6"
                        }}
                    ],
                    "channels": 32
                }},
                {{
                    "blocks": [
                        {{
                            "type": "MBConv",
                            "kernel_size": 7,
                            "expansion": 4,
                            "has_se": true,
                            "se_ratios": 0.25,
                            "skip_connection": true,
                            "stride": 1,
                            "activation": "Swish"
                        }}
                    ],
                    "channels": 32
                }}
            ],
            "constraints": {{
                "max_sram": 1953.125,
                "min_macs": 0.2,
                "max_macs": 20.0,
                "max_params": 5.0,
                "max_peak_memory": 200.0,
                "max_latency": 100
            }}
        }}""".format(
                constraints=json.dumps(constraints, indent=2),
                search_space=json.dumps(self.search_space['search_space'], indent=2),
                feedback=feedback or "No Pareto frontier feedback",
                failure_feedback=failure_feedback or "None",
                dataset_name=dataset_name,
                channels=dataset_info['channels'],
                time_steps=dataset_info['time_steps'],
                num_classes=dataset_info['num_classes'],
                description=dataset_info['description']
            )
        # æ„å»ºå®Œæ•´æç¤º
        # print(f"æ„å»ºçš„æç¤º:\n{search_prompt}...\n{'-'*50}")
       
        return search_prompt
    
    def _parse_response(self, response: str) -> Optional[CandidateModel]:
        """è§£æLLMå“åº”ä¸ºå€™é€‰æ¨¡å‹"""
        try:
            # å°è¯•è§£æJSONå“åº”
            json_match = re.search(r'```json(.*?)```', response, re.DOTALL)
            if json_match:
                json_str = json_match.group(1).strip()
                # print(f"æå–çš„JSONå­—ç¬¦ä¸²:\n{json_str}")
                config = json5.loads(json_str)
            else:
                json_match = re.search(r'```(.*?)```', response, re.DOTALL)
                # print(f"æå–çš„JSONå­—ç¬¦ä¸²:\n{json_str}")
                config = json5.loads(json_str)
            # print(f"è§£æå‡ºçš„é…ç½®:\n{json.dumps(config, indent=2)}")

            # åŸºæœ¬é…ç½®éªŒè¯
            if not all(k in config for k in ['stages', 'constraints']):
                raise ValueError("é…ç½®ç¼ºå°‘å¿…è¦å­—æ®µ(stages æˆ– constraints)")

            # ç¡®ä¿æ‰€æœ‰æ•°å€¼å­—æ®µéƒ½æ˜¯æ•°å­—ç±»å‹
            def convert_numbers(obj):
                if isinstance(obj, dict):
                    return {k: convert_numbers(v) for k, v in obj.items()}
                elif isinstance(obj, list):
                    return [convert_numbers(v) for v in obj]
                elif isinstance(obj, str):
                    try:
                        return float(obj) if '.' in obj else int(obj)
                    except ValueError:
                        return obj
                return obj

            config = convert_numbers(config)
            
            # åˆ›å»ºå€™é€‰æ¨¡å‹å®ä¾‹
            candidate = CandidateModel(config=config)
            # åˆ›å»ºå€™é€‰æ¨¡å‹å®ä¾‹ï¼ˆä¸å†éªŒè¯çº¦æŸï¼‰
            return CandidateModel(config=config)

            
        except json.JSONDecodeError:
            print(f"æ— æ³•è§£æLLMå“åº”ä¸ºJSON: {response}")
            return None
        except Exception as e:
            print(f"é…ç½®è§£æå¤±è´¥: {str(e)}")
            return None


    def run_search(self, iterations: int = 100) -> Dict:
        """
        è¿è¡Œå®Œæ•´çš„æœç´¢æµç¨‹
        
        å‚æ•°:
            iterations: æœç´¢è¿­ä»£æ¬¡æ•°
        è¿”å›:
            åŒ…å«æœ€ä½³æ¨¡å‹å’ŒParetoå‰æ²¿çš„å­—å…¸
        """
        
        # è·å–æ­£ç¡®çš„æ•°æ®é›†ä¿¡æ¯
        # input_shape = (1, dataset_info['channels'], dataset_info['time_steps'])  # æ­£ç¡®çš„è¾“å…¥å°ºå¯¸

        dataloaders = get_multitask_dataloaders('/root/tinyml/data')

        # æˆ–è€…ä½¿ç”¨æœ€å¤§æ—¶é—´æ­¥é•¿ï¼ˆç¡®ä¿æ¨¡å‹èƒ½å¤„ç†æ‰€æœ‰æ•°æ®é›†ï¼‰
        # max_time_steps = max(info['time_steps'] for info in self.dataset_info.values())
        # input_shape = (1, 6, max_time_steps)  # 6æ˜¯æ‰€æœ‰æ•°æ®é›†çš„é€šé“æ•°

        results = {
            'best_models': [],
            'pareto_front': []
        }

        best_models = []

        # è®¾ç½®ä¸­å›½æ ‡å‡†æ—¶é—´ï¼ˆUTC+8ï¼‰
        china_timezone = pytz.timezone("Asia/Shanghai")
        # ç¡®ä¿ä¸»ä¿å­˜ç›®å½•å­˜åœ¨
        base_save_dir = "/root/tinyml/weights/tinyml"
        os.makedirs(base_save_dir, exist_ok=True)

         # åˆ›å»ºä¸€ä¸ªå”¯ä¸€çš„æ—¶é—´æˆ³å­æ–‡ä»¶å¤¹
        timestamp = datetime.now(china_timezone).strftime("%m-%d-%H-%M")  # æ ¼å¼ä¸º "æœˆ-æ—¥-æ—¶-åˆ†"
        run_save_dir = os.path.join(base_save_dir, timestamp)
        os.makedirs(run_save_dir, exist_ok=True)  # ç¡®ä¿å­æ–‡ä»¶å¤¹å­˜åœ¨

        print(f"æ‰€æœ‰æ¨¡å‹å°†ä¿å­˜åˆ°ç›®å½•: {run_save_dir}")
        
        # åˆå§‹åŒ–ç»“æœå­—å…¸
        overall_results = {}

        # éå†æ¯ä¸ªæ•°æ®é›†
        for dataset_name in self.dataset_names:
            print(f"\n{'='*30} å¼€å§‹æœç´¢æ•°æ®é›†: {dataset_name} {'='*30}")

            # é‡ç½® Pareto å‰æ²¿ï¼Œç¡®ä¿æ¯ä¸ªä»»åŠ¡ä»é›¶å¼€å§‹
            self.pareto_front.reset()

            # åˆå§‹åŒ–æ¯ä¸ªæ•°æ®é›†çš„ç»“æœ
            dataset_results = {
                'best_models': [],
                'pareto_front': []
            }

            # ä¸ºå½“å‰æ•°æ®é›†åˆ›å»ºç‹¬ç«‹çš„ä¿å­˜ç›®å½•
            dataset_save_dir = os.path.join(run_save_dir, dataset_name)
            os.makedirs(dataset_save_dir, exist_ok=True)

            # è·å–å½“å‰æ•°æ®é›†çš„æ•°æ®åŠ è½½å™¨
            dataloader = dataloaders[dataset_name]
            # ä¸ºå½“å‰æ•°æ®é›†è¿è¡Œ `iterations` æ¬¡æœç´¢

            input_shape = (1, self.dataset_info[dataset_name]['channels'], self.dataset_info[dataset_name]['time_steps'])  # ç¡®ä¿è¾“å…¥å½¢çŠ¶æ­£ç¡®

            for i in range(iterations):
                logger.info(f"\n{'-'*30} æ•°æ®é›† {dataset_name} - è¿­ä»£ {i+1}/{iterations} {'-'*30}")
                
                # ç”Ÿæˆå€™é€‰æ¶æ„
                candidate = self.generate_candidate(dataset_name)
                if candidate is None:
                    continue
                
                # è¯„ä¼°å€™é€‰æ¶æ„
                try:
                    # æ„å»ºæ¨¡å‹
                    model = candidate.build_model()
                    print("âœ… æ¨¡å‹æ„å»ºæˆåŠŸ")
                    # éªŒè¯æ¨¡å‹è¾“å‡ºç»´åº¦
                    if not hasattr(model, 'output_dim'):
                        raise AttributeError("Built model missing 'output_dim' attribute")
                    print(f"æ¨¡å‹è¾“å‡ºç»´åº¦: {model.output_dim}")

                    try:
                        from torchinfo import summary
                        summary(model, input_size=input_shape)
                    except ImportError:
                        print("âš ï¸ æœªå®‰è£…torchinfoï¼Œ æ— æ³•æ‰“å°æ¨¡å‹ç»“æ„")
                        print("æ¨¡å‹ç»“æ„:", model)

                    # è®­ç»ƒå¹¶è¯„ä¼°æ¨¡å‹
                    # trainer = MultiTaskTrainer(model, dataloaders)
                    # åˆ›å»ºè®­ç»ƒå™¨
                    trainer = SingleTaskTrainer(model, dataloader)

                    # ä¸ºæ¯ä¸ªå€™é€‰æ¨¡å‹ç”Ÿæˆå”¯ä¸€çš„ä¿å­˜è·¯å¾„
                    save_path = os.path.join(dataset_save_dir, f"best_model_iter_{i+1}.pth")

                    # è®­ç»ƒæ¨¡å‹å¹¶ä¿å­˜æœ€ä½³æƒé‡
                    best_acc, best_val_metrics, history, best_state = trainer.train(epochs=10, save_path=save_path)  # å¿«é€Ÿè®­ç»ƒ5ä¸ªepoch

                    # ä½¿ç”¨æœ€ä½³å‡†ç¡®ç‡ä½œä¸ºå€™é€‰æ¨¡å‹çš„å‡†ç¡®ç‡
                    candidate.accuracy = best_acc
                    # candidate.val_accuracy = {k: v['accuracy'] / 100 for k, v in best_val_metrics.items()}  # ä¿å­˜æœ€ä½³éªŒè¯å‡†ç¡®ç‡
                    candidate.val_accuracy = best_val_metrics['accuracy'] / 100  # ä¿å­˜æœ€ä½³éªŒè¯å‡†ç¡®ç‡
                    candidate.metadata['best_model_path'] = save_path  # ä¿å­˜æœ€ä½³æƒé‡è·¯å¾„
                    # æµ‹é‡å³°å€¼å†…å­˜ï¼ˆGPUï¼‰
                    peak_memory_mb = candidate.measure_peak_memory(device='cuda', dataset_names=dataset_name)
                    print(f"Peak Memory Usage: {peak_memory_mb:.2f} MB")

                    # æµ‹é‡æ¨ç†æ—¶å»¶ï¼ˆGPUï¼‰
                    latency_ms = candidate.measure_latency(device='cuda', dataset_names=dataset_name)
                    print(f"â±ï¸ Inference Latency: {latency_ms:.2f} ms")
                    
                    # åˆ†æè®­ç»ƒç»“æœ
                    print("\n=== è®­ç»ƒç»“æœ ===")
                    # print(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_acc:.2%}")
                    
                    for epoch, record in enumerate(history):
                        print(f"\nEpoch {epoch+1}:")
                        print(f"è®­ç»ƒå‡†ç¡®ç‡: {record['train']['accuracy']:.2f}%")
                        print(f"éªŒè¯å‡†ç¡®ç‡: {record['val']['accuracy']:.2f}%")

                    print("\nâœ… è®­ç»ƒæµ‹è¯•å®Œæˆ ")
            

                    # è®¡ç®—æŒ‡æ ‡
                    metrics = {
                        'macs': candidate.estimate_macs(),
                        'params': candidate.estimate_params(),
                        # è¿™ä¸ªåœ°æ–¹ç»å¯¹é”™è¯¯
                        'sram': MemoryEstimator.calc_model_sram(candidate),
                        # è¿™é‡Œéœ€è¦æ·»åŠ å®é™…è¯„ä¼°å‡†ç¡®ç‡çš„æ–¹æ³•
                        'accuracy': best_acc,
                        'val_accuracy': candidate.val_accuracy,
                        'latency': latency_ms,  # æ–°å¢latencyæŒ‡æ ‡
                        'peak_memory': peak_memory_mb  # æ–°å¢å³°å€¼å†…å­˜æŒ‡æ ‡
                    }
                    # print(f"å€™é€‰æŒ‡æ ‡: {metrics}")

                    # æ›´æ–°Paretoå‰æ²¿
                    if self.pareto_front.update(candidate, metrics):
                        print("âœ… æ–°å€™é€‰åŠ å…¥Paretoå‰æ²¿")
                    
                    # è®°å½•æœ€ä½³æ¨¡å‹
                    if self.pareto_front.is_best(candidate):
                        best_models.append(candidate)
                        print("ğŸ† æ–°çš„æœ€ä½³æ¨¡å‹!")
                except Exception as e:
                    print(f"æ¨¡å‹è¯„ä¼°å¤±è´¥: {str(e)}")
                    continue

            # # æ‰“å° Pareto å‰æ²¿ä¸­çš„æ‰€æœ‰æ¨¡å‹ä¿¡æ¯
            print("\n=== Pareto Front Summary ===")
            pareto_info = []  # ç”¨äºä¿å­˜Paretoå‰æ²¿ä¿¡æ¯
            for i, candidate in enumerate(self.pareto_front.get_front(), 1):
                model_info = {
                    "index": i,
                    "accuracy": float(candidate.accuracy),
                    "macs": float(candidate.macs),
                    "params": float(candidate.params),
                    "sram": float(candidate.sram) / 1e3,
                    "latency": float(candidate.latency),
                    "peak_memory": float(candidate.peak_memory),  # è½¬æ¢ä¸ºKB
                    "val_accuracy": candidate.val_accuracy,
                    "best_model_path": candidate.metadata.get('best_model_path', 'N/A'),
                    "configuration": candidate.config
                }
                pareto_info.append(model_info)
                
                print(f"\nPareto Model #{i}:")
                print(f"- Accuracy: {candidate.accuracy:.2f}%")
                print(f"- MACs: {candidate.macs:.2f}M")
                print(f"- Parameters: {candidate.params:.2f}M")
                print(f"- SRAM: {candidate.sram / 1e3:.2f}KB")
                print(f"- Latency: {candidate.latency:.2f} ms")
                print(f"- Peak Memory: {candidate.peak_memory:.2f} MB")
                # print(f"- Validation Accuracy by Task: {json.dumps(candidate.val_accuracy, indent=2)}")
                print(f"- Validation Accuracy: {candidate.val_accuracy:.2%}")
                print(f"- Best Model Path: {candidate.metadata.get('best_model_path', 'N/A')}")
                print(f"- Configuration: {json.dumps(candidate.config, indent=2)}")

            # ä¿å­˜Paretoå‰æ²¿ä¿¡æ¯åˆ°JSONæ–‡ä»¶
            pareto_save_path = os.path.join(dataset_save_dir, "pareto_front.json")
            try:
                with open(pareto_save_path, 'w', encoding='utf-8') as f:
                    json.dump(pareto_info, f, indent=2, ensure_ascii=False)
                print(f"\nâœ… Pareto å‰æ²¿ä¿¡æ¯å·²ä¿å­˜åˆ°: {pareto_save_path}")
            except Exception as e:
                print(f"\nâŒ ä¿å­˜ Pareto å‰æ²¿ä¿¡æ¯å¤±è´¥: {str(e)}")

            # å°†å½“å‰æ•°æ®é›†çš„ç»“æœå­˜å‚¨åˆ°æ•´ä½“ç»“æœä¸­
            dataset_results['pareto_front'] = self.pareto_front.get_front()
            overall_results[dataset_name] = dataset_results

        return overall_results


# ç¤ºä¾‹ç”¨æ³•
if __name__ == "__main__":
    
    # # åˆ›å»ºæœç´¢å™¨å®ä¾‹
    # searcher = LLMGuidedSearcher(llm_config["llm"], search_space)
    
    # # è¿è¡Œæœç´¢
    # results = searcher.run_search(iterations=2)

    # # æ‰“å°æ¯ä¸ªæ•°æ®é›†çš„ Pareto å‰æ²¿æ¨¡å‹æ•°é‡
    # for dataset_name, dataset_results in results.items():
    #     pareto_count = len(dataset_results['pareto_front'])
    #     print(f"æ•°æ®é›† {dataset_name} çš„ Pareto å‰æ²¿æ¨¡å‹æ•°é‡: {pareto_count}")




    try:
        # ä¿®æ”¹é…ç½®ä¸ºä¸€ä¸ªç®€å•çš„æ¨¡å‹ï¼ˆåªæœ‰ä¸€ä¸ª stageï¼‰
        simple_config = {
            "input_channels": 6,  # har70plus çš„è¾“å…¥é€šé“æ•°
            "num_classes": 7,  # har70plus çš„ç±»åˆ«æ•°
            "stages": [
                {
                    "blocks": [
                        {
                            "type": "DWSepConv",
                            "kernel_size": 3,
                            "expansion": 1,
                            "has_se": False,
                            "se_ratios": 0,
                            "skip_connection": False,
                            "stride": 1,
                            "activation": "ReLU6"
                        }
                    ],
                    "channels": 8  # stage çš„è¾“å‡ºé€šé“æ•°
                }
            ],
            "constraints": {
                "max_sram": 1953.125,
                "min_macs": 0.2,
                "max_macs": 20.0,
                "max_params": 5.0,
                "max_peak_memory": 200.0,
                "max_latency": 100
            }
        }
        # é…ç½® 2 ä¸ª stage
        config_2_stages = {
            "input_channels": 6,  # har70plus çš„è¾“å…¥é€šé“æ•°
            "num_classes": 7,  # har70plus çš„ç±»åˆ«æ•°
            "stages": [
                {
                    "blocks": [
                        {
                            "type": "DWSepConv",
                            "kernel_size": 3,
                            "expansion": 1,
                            "has_se": True,
                            "se_ratios": 0.25,
                            "skip_connection": False,
                            "stride": 1,
                            "activation": "ReLU6"
                        }
                    ],
                    "channels": 8  # stage çš„è¾“å‡ºé€šé“æ•°
                }
            ],
            "constraints": {
                "max_sram": 1953.125,
                "min_macs": 0.2,
                "max_macs": 20.0,
                "max_params": 5.0,
                "max_peak_memory": 200.0,
                "max_latency": 100
            }
        }

        # é…ç½® 3 ä¸ª stage
        config_3_stages = {
            "input_channels": 6,  # har70plus çš„è¾“å…¥é€šé“æ•°
            "num_classes": 7,  # har70plus çš„ç±»åˆ«æ•°
            "stages": [
                {
                    "blocks": [
                        {
                            "type": "DWSepConv",
                            "kernel_size": 3,
                            "expansion": 1,
                            "has_se": True,
                            "se_ratios": 0.5,
                            "skip_connection": False,
                            "stride": 1,
                            "activation": "ReLU6"
                        }
                    ],
                    "channels": 8  # stage çš„è¾“å‡ºé€šé“æ•°
                }
            ],
            "constraints": {
                "max_sram": 1953.125,
                "min_macs": 0.2,
                "max_macs": 20.0,
                "max_params": 5.0,
                "max_peak_memory": 200.0,
                "max_latency": 100
            }
        }
   
        # æµ‹è¯•æ€§èƒ½å‡½æ•°ï¼ˆåŒ…æ‹¬è®­ç»ƒå¹¶è®¡ç®—å‡†ç¡®ç‡ï¼‰
        def test_model_with_training(config, description, dataloader, save_dir, epochs=20):
            """
            æµ‹è¯•æ¨¡å‹çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬è®­ç»ƒå¹¶è®¡ç®—å‡†ç¡®ç‡
            å‚æ•°:
                config: æ¨¡å‹é…ç½®
                description: æ¨¡å‹æè¿°
                dataloader: æ•°æ®åŠ è½½å™¨
                save_dir: ä¿å­˜æƒé‡çš„ç›®å½•
                epochs: è®­ç»ƒçš„epochæ•°
            """
            print(f"\n=== æµ‹è¯•æ¨¡å‹: {description} ===")
            candidate = CandidateModel(config=config)

            # æ‰“å°æ¨¡å‹é…ç½®
            print("\n=== æ¨¡å‹é…ç½® ===")
            print(json.dumps(config, indent=2))

            # æ„å»ºæ¨¡å‹
            model = candidate.build_model()
            print("âœ… æ¨¡å‹æ„å»ºæˆåŠŸ")

            # éªŒè¯æ¨¡å‹è¾“å‡ºç»´åº¦
            if not hasattr(model, 'output_dim'):
                raise AttributeError("æ„å»ºçš„æ¨¡å‹ç¼ºå°‘ 'output_dim' å±æ€§")

            # æ‰“å°æ¨¡å‹ç»“æ„
            try:
                from torchinfo import summary
                summary(model, input_size=(1, config['input_channels'], 500))  # å‡è®¾è¾“å…¥æ—¶é—´æ­¥é•¿ä¸º500
            except ImportError:
                print("âš ï¸ æœªå®‰è£…torchinfoï¼Œæ— æ³•æ‰“å°æ¨¡å‹ç»“æ„")
                print("æ¨¡å‹ç»“æ„:", model)

            # åˆ›å»ºè®­ç»ƒå™¨
            trainer = SingleTaskTrainer(model, dataloader)

            # ä¸ºå½“å‰æ¨¡å‹ç”Ÿæˆå”¯ä¸€çš„ä¿å­˜è·¯å¾„
            save_path = os.path.join(save_dir, f"{description.replace(' ', '_')}_best_model.pth")

            # è®­ç»ƒæ¨¡å‹
            print(f"å¼€å§‹è®­ç»ƒæ¨¡å‹: {description}")
            best_acc, best_val_metrics, history, best_state = trainer.train(epochs=epochs, save_path=save_path)

            # ä½¿ç”¨æœ€ä½³å‡†ç¡®ç‡ä½œä¸ºå€™é€‰æ¨¡å‹çš„å‡†ç¡®ç‡
            candidate.accuracy = best_acc
            candidate.val_accuracy = best_val_metrics['accuracy'] / 100  # ä¿å­˜æœ€ä½³éªŒè¯å‡†ç¡®ç‡

            # æµ‹è¯•å»¶è¿Ÿ
            latency_ms = candidate.measure_latency(device='cuda', dataset_names='har70plus')
            print(f"â±ï¸ æ¨ç†å»¶è¿Ÿ: {latency_ms:.2f} ms")

            # æµ‹è¯•å³°å€¼å†…å­˜
            peak_memory_mb = candidate.measure_peak_memory(device='cuda', dataset_names='har70plus')
            print(f"å³°å€¼å†…å­˜ä½¿ç”¨: {peak_memory_mb:.2f} MB")

            # æ‰“å°è®­ç»ƒç»“æœ
            print("\n=== è®­ç»ƒç»“æœ ===")
            print(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_acc:.2%}")
            for epoch, record in enumerate(history):
                print(f"\nEpoch {epoch+1}:")
                print(f"è®­ç»ƒå‡†ç¡®ç‡: {record['train']['accuracy']:.2f}%")
                print(f"éªŒè¯å‡†ç¡®ç‡: {record['val']['accuracy']:.2f}%")

            print("\nâœ… æ¨¡å‹æµ‹è¯•å®Œæˆ")

            # è¿”å›å€™é€‰æ¨¡å‹çš„æ€§èƒ½æŒ‡æ ‡
            return {
                "description": description,
                "accuracy": best_acc,
                "val_accuracy": candidate.val_accuracy,
                "latency": latency_ms,
                "peak_memory": peak_memory_mb,
                "config": config
            }

        # åŠ è½½æ•°æ®é›†
        dataloaders = get_multitask_dataloaders('/root/tinyml/data')
        dataloader = dataloaders['har70plus']  # ä½¿ç”¨ har70plus æ•°æ®é›†

        # è®¾ç½®ä¿å­˜ç›®å½•
        save_dir = "/root/tinyml/weights/tinyml/test_models"
        os.makedirs(save_dir, exist_ok=True)

        # æµ‹è¯•æ¨¡å‹
        results = []
        results.append(test_model_with_training(simple_config, "1 ä¸ª stage", dataloader, save_dir, epochs=20))
        results.append(test_model_with_training(config_2_stages, "2 ä¸ª stage", dataloader, save_dir, epochs=20))
        results.append(test_model_with_training(config_3_stages, "3 ä¸ª stage", dataloader, save_dir, epochs=20))

        # æ‰“å°ç»“æœ
        print("\n=== æµ‹è¯•ç»“æœ ===")
        for result in results:
            print(f"\næ¨¡å‹æè¿°: {result['description']}")
            print(f"å‡†ç¡®ç‡: {result['accuracy']:.2%}")
            print(f"éªŒè¯å‡†ç¡®ç‡: {result['val_accuracy']:.2%}")
            print(f"æ¨ç†å»¶è¿Ÿ: {result['latency']:.2f} ms")
            print(f"å³°å€¼å†…å­˜ä½¿ç”¨: {result['peak_memory']:.2f} MB")
            print(f"æ¨¡å‹é…ç½®: {json.dumps(result['config'], indent=2)}")

        
        
        
        # æµ‹è¯•æ€§èƒ½å‡½æ•°
        def test_model(config, description):
            print(f"\n=== æµ‹è¯•æ¨¡å‹: {description} ===")
            candidate = CandidateModel(config=config)

            # æ‰“å°æ¨¡å‹é…ç½®
            print("\n=== æ¨¡å‹é…ç½® ===")
            print(json.dumps(config, indent=2))

            # æµ‹è¯•å»¶è¿Ÿ
            latency_ms = candidate.measure_latency(device='cuda', dataset_names='har70plus')
            print(f"â±ï¸ æ¨ç†å»¶è¿Ÿ: {latency_ms:.2f} ms")

            # æµ‹è¯•å³°å€¼å†…å­˜
            peak_memory_mb = candidate.measure_peak_memory(device='cuda', dataset_names='har70plus')
            print(f"å³°å€¼å†…å­˜ä½¿ç”¨: {peak_memory_mb:.2f} MB")



        # # æµ‹è¯• 1 ä¸ª stage çš„æ¨¡å‹
        # test_model(simple_config, "1 ä¸ª stage")
        # # æµ‹è¯• 2 ä¸ª stage çš„æ¨¡å‹
        # test_model(config_2_stages, "2 ä¸ª stage")

        # # æµ‹è¯• 3 ä¸ª stage çš„æ¨¡å‹
        # test_model(config_3_stages, "3 ä¸ª stage")


    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        
